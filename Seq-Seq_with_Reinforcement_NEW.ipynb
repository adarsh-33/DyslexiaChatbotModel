{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4830d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "#print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "079cb69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'D:\\Anaconda3\\envs\\env_keras\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\pip-install-1ncrds66\\\\pyahocorasick_0aef2f25e9fc49b6adfd7e52673cc450\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\pip-install-1ncrds66\\\\pyahocorasick_0aef2f25e9fc49b6adfd7e52673cc450\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-wheel-9wzcputr'\n",
      "       cwd: C:\\Users\\user\\AppData\\Local\\Temp\\pip-install-1ncrds66\\pyahocorasick_0aef2f25e9fc49b6adfd7e52673cc450\\\n",
      "  Complete output (5 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'ahocorasick' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for pyahocorasick\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'D:\\Anaconda3\\envs\\env_keras\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\pip-install-1ncrds66\\\\pyahocorasick_0aef2f25e9fc49b6adfd7e52673cc450\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\pip-install-1ncrds66\\\\pyahocorasick_0aef2f25e9fc49b6adfd7e52673cc450\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-record-yr7ubly1\\install-record.txt' --single-version-externally-managed --compile --install-headers 'D:\\Anaconda3\\envs\\env_keras\\Include\\pyahocorasick'\n",
      "         cwd: C:\\Users\\user\\AppData\\Local\\Temp\\pip-install-1ncrds66\\pyahocorasick_0aef2f25e9fc49b6adfd7e52673cc450\\\n",
      "    Complete output (7 lines):\n",
      "    running install\n",
      "    D:\\Anaconda3\\envs\\env_keras\\lib\\site-packages\\setuptools\\command\\install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "      setuptools.SetuptoolsDeprecationWarning,\n",
      "    running build\n",
      "    running build_ext\n",
      "    building 'ahocorasick' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'D:\\Anaconda3\\envs\\env_keras\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\pip-install-1ncrds66\\\\pyahocorasick_0aef2f25e9fc49b6adfd7e52673cc450\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\pip-install-1ncrds66\\\\pyahocorasick_0aef2f25e9fc49b6adfd7e52673cc450\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-record-yr7ubly1\\install-record.txt' --single-version-externally-managed --compile --install-headers 'D:\\Anaconda3\\envs\\env_keras\\Include\\pyahocorasick' Check the logs for full command output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Using cached textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii\n",
      "  Using cached anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "Collecting pyahocorasick\n",
      "  Using cached pyahocorasick-2.0.0.tar.gz (99 kB)\n",
      "Building wheels for collected packages: pyahocorasick\n",
      "  Building wheel for pyahocorasick (setup.py): started\n",
      "  Building wheel for pyahocorasick (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pyahocorasick\n",
      "Failed to build pyahocorasick\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "    Running setup.py install for pyahocorasick: started\n",
      "    Running setup.py install for pyahocorasick: finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d09f87d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ca24011cd262>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.util import ngrams\n",
    "\n",
    "data_path = 'data.txt'\n",
    "\n",
    "input_texts = []\n",
    "output_texts = []\n",
    "with open(data_path) as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(600, len(lines) - 1)]:\n",
    "    input_text = line.split('\\t')[0]\n",
    "    target_text = line.split('\\t')[1]\n",
    "    input_texts.append(input_text)\n",
    "    output_texts.append(target_text)\n",
    "\n",
    "\n",
    "import contractions\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_tokens = text.split()\n",
    "    filtered_text = [word for word in text_tokens if word not in stop_words]\n",
    "    text = ' '.join(filtered_text)\n",
    "    # Lemmatize words\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    # text_tokens = text.split()\n",
    "    # lemmatized_text = [lemmatizer.lemmatize(word) for word in text_tokens]\n",
    "    # text = ' '.join(lemmatized_text)\n",
    "    return text\n",
    "\n",
    "# Preprocess input and output sequences\n",
    "input_texts = [preprocess_text(text) for text in input_texts]\n",
    "output_texts = [preprocess_text(text) for text in output_texts]\n",
    "\n",
    "# Define the n-gram order\n",
    "n = 2\n",
    "\n",
    "# Generate n-gram sequences\n",
    "input_ngram_sequences = []\n",
    "output_ngram_sequences = []\n",
    "\n",
    "for text in input_texts:\n",
    "    ngrams_sequence = [' '.join(ngram) for ngram in ngrams(text.split(), n)]\n",
    "    input_ngram_sequences.append(' '.join(ngrams_sequence))\n",
    "\n",
    "for text in output_texts:\n",
    "    ngrams_sequence = [' '.join(ngram) for ngram in ngrams(text.split(), n)]\n",
    "    output_ngram_sequences.append(' '.join(ngrams_sequence))\n",
    "\n",
    "# Configure the Tokenizer with n-grams support\n",
    "tokenizer = Tokenizer(lower=True, filters='', split=' ')\n",
    "tokenizer.fit_on_texts(input_ngram_sequences + output_ngram_sequences)\n",
    "\n",
    "# Convert text sequences to integer sequences\n",
    "input_sequences = tokenizer.texts_to_sequences(input_ngram_sequences)\n",
    "output_sequences = tokenizer.texts_to_sequences(output_ngram_sequences)\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_seq_length = max(len(seq) for seq in input_sequences + output_sequences)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length, padding='post')\n",
    "output_sequences = pad_sequences(output_sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Define encoder input, decoder input, and decoder output\n",
    "encoder_input = input_sequences\n",
    "decoder_input = np.zeros_like(output_sequences)\n",
    "decoder_input[:, 1:] = output_sequences[:, :-1]\n",
    "decoder_output = np.eye(vocab_size)[output_sequences]\n",
    "\n",
    "# Define the Seq2Seq model\n",
    "latent_dim = 128\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(vocab_size, latent_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, dropout=dropout_rate, recurrent_dropout=dropout_rate)\n",
    "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "encoder_dropout = Dropout(dropout_rate)\n",
    "encoder_dropout_output = encoder_dropout(state_h)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(vocab_size, latent_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=dropout_rate, recurrent_dropout=dropout_rate)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dropout = Dropout(dropout_rate)\n",
    "decoder_dropout_outputs = decoder_dropout(decoder_outputs)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_dropout_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Define K-fold cross-validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Define RLHF parameters\n",
    "rl_epochs = 5  # Number of RL epochs\n",
    "rl_learning_rate = 0.001  # RL learning rate\n",
    "rl_batch_size = 64  # RL batch size\n",
    "reinforce_reward = 1  # Reward value for positive reinforcement\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(input_sequences)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    x_train, x_val = input_sequences[train_indices], input_sequences[val_indices]\n",
    "    y_train, y_val = decoder_output[train_indices], decoder_output[val_indices]\n",
    "    decoder_input_train, decoder_input_val = decoder_input[train_indices], decoder_input[val_indices]\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define early stopping callback\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    # Train the model using supervised learning\n",
    "    history = model.fit([x_train, decoder_input_train], y_train,\n",
    "                        validation_data=([x_val, decoder_input_val], y_val),\n",
    "                        epochs=10, batch_size=64, callbacks=[early_stop])\n",
    "\n",
    "    # Apply RLHF\n",
    "    for rl_epoch in range(rl_epochs):\n",
    "        # Generate translations using the current model\n",
    "        predictions = model.predict([x_train, decoder_input_train])\n",
    "        decoded_sequences = []\n",
    "        for prediction in predictions:\n",
    "            decoded_sequence = []\n",
    "            for token in prediction:\n",
    "                sampled_token_index = np.argmax(token)\n",
    "                sampled_token = tokenizer.index_word.get(sampled_token_index, '')\n",
    "                if sampled_token != '':\n",
    "                    decoded_sequence.append(sampled_token)\n",
    "            decoded_sequences.append(decoded_sequence)\n",
    "\n",
    "        # Calculate rewards based on BLEU score or other metrics\n",
    "        # rewards = calculate_rewards(decoded_sequences, ground_truth_sequences)\n",
    "\n",
    "        # Prepare RL data by converting sequences to integer sequences\n",
    "        rl_input_sequences = tokenizer.texts_to_sequences(decoded_sequences)\n",
    "        rl_input_sequences = pad_sequences(rl_input_sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "        # Train the model using RL\n",
    "        model.optimizer.lr.assign(rl_learning_rate)\n",
    "        model.fit([x_train, rl_input_sequences], y_train, epochs=1, batch_size=rl_batch_size)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate([x_val, decoder_input_val], y_val)\n",
    "    print(test_loss)\n",
    "    print(test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"RL_seq2seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e8dbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 349ms/step\n",
      "Input: How are you?\n",
      "Response: \n",
      "\n",
      "1/1 [==============================] - 0s 309ms/step\n",
      "Input: What is your name?\n",
      "Response: \n",
      "\n",
      "1/1 [==============================] - 0s 308ms/step\n",
      "Input: Can you help me?\n",
      "Response: \n",
      "\n",
      "1/1 [==============================] - 0s 308ms/step\n",
      "Input: Where are you from?\n",
      "Response: \n",
      "\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "Input: What is the weather like today?\n",
      "Response: chatbot chatbot chatbot\n",
      "\n",
      "1/1 [==============================] - 0s 307ms/step\n",
      "Input: what is dyslexia\n",
      "Response: \n",
      "\n",
      "1/1 [==============================] - 0s 304ms/step\n",
      "Input: When were you born?\n",
      "Response: \n",
      "\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "Input: it possible to identify dyslexia in a youngster who has hearing loss?\n",
      "Response: assist help dyslexia dyslexia dyslexia dyslexia dyslexia dyslexia\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a function to preprocess a single input text\n",
    "def preprocess_input(text):\n",
    "    # Implement the same preprocessing steps as in the training code\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_tokens = text.split()\n",
    "    filtered_text = [word for word in text_tokens if word not in stop_words]\n",
    "    text = ' '.join(filtered_text)\n",
    "    return text\n",
    "\n",
    "# Define a function to generate a response given an input text\n",
    "def generate_response(input_text):\n",
    "    preprocessed_text = preprocess_input(input_text)\n",
    "    ngrams_sequence = [' '.join(ngram) for ngram in ngrams(preprocessed_text.split(), n)]\n",
    "    input_ngram_sequence = ' '.join(ngrams_sequence)\n",
    "    input_sequence = tokenizer.texts_to_sequences([input_ngram_sequence])\n",
    "    padded_sequence = pad_sequences(input_sequence, maxlen=max_seq_length, padding='post')\n",
    "    decoder_input = np.zeros((1, max_seq_length))\n",
    "    decoder_input[:, 1:] = padded_sequence[:, :-1]\n",
    "    predictions = model.predict([padded_sequence, decoder_input])\n",
    "    decoded_sequence = []\n",
    "    for token in predictions[0]:\n",
    "        sampled_token_index = np.argmax(token)\n",
    "        sampled_token = tokenizer.index_word.get(sampled_token_index, '')\n",
    "        if sampled_token != '':\n",
    "            decoded_sequence.append(sampled_token)\n",
    "    response = ' '.join(decoded_sequence)\n",
    "    return response\n",
    "\n",
    "# Test the model on new input texts\n",
    "input_texts_test = [\n",
    "    \"How are you?\",\n",
    "    \"What is your name?\",\n",
    "    \"Can you help me?\",\n",
    "    \"Where are you from?\",\n",
    "    \"What is the weather like today?\",\n",
    "    \"what is dyslexia\",\n",
    "    \"When were you born?\",\n",
    "    \"it possible to identify dyslexia in a youngster who has hearing loss?\"\n",
    "]\n",
    "\n",
    "for input_text in input_texts_test:\n",
    "    response = generate_response(input_text)\n",
    "    print(\"Input:\", input_text)\n",
    "    print(\"Response:\", response)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d9d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
