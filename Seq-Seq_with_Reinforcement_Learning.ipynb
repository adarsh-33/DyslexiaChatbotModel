{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4830d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "#print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079cb69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f87d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.util import ngrams\n",
    "\n",
    "data_path = 'data.txt'\n",
    "\n",
    "input_texts = []\n",
    "output_texts = []\n",
    "with open(data_path) as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(600, len(lines) - 1)]:\n",
    "    input_text = line.split('\\t')[0]\n",
    "    target_text = line.split('\\t')[1]\n",
    "    input_texts.append(input_text)\n",
    "    output_texts.append(target_text)\n",
    "\n",
    "\n",
    "import contractions\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_tokens = text.split()\n",
    "    filtered_text = [word for word in text_tokens if word not in stop_words]\n",
    "    text = ' '.join(filtered_text)\n",
    "    # Lemmatize words\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    # text_tokens = text.split()\n",
    "    # lemmatized_text = [lemmatizer.lemmatize(word) for word in text_tokens]\n",
    "    # text = ' '.join(lemmatized_text)\n",
    "    return text\n",
    "\n",
    "# Preprocess input and output sequences\n",
    "input_texts = [preprocess_text(text) for text in input_texts]\n",
    "output_texts = [preprocess_text(text) for text in output_texts]\n",
    "\n",
    "# Define the n-gram order\n",
    "n = 2\n",
    "\n",
    "# Generate n-gram sequences\n",
    "input_ngram_sequences = []\n",
    "output_ngram_sequences = []\n",
    "\n",
    "for text in input_texts:\n",
    "    ngrams_sequence = [' '.join(ngram) for ngram in ngrams(text.split(), n)]\n",
    "    input_ngram_sequences.append(' '.join(ngrams_sequence))\n",
    "\n",
    "for text in output_texts:\n",
    "    ngrams_sequence = [' '.join(ngram) for ngram in ngrams(text.split(), n)]\n",
    "    output_ngram_sequences.append(' '.join(ngrams_sequence))\n",
    "\n",
    "# Configure the Tokenizer with n-grams support\n",
    "tokenizer = Tokenizer(lower=True, filters='', split=' ')\n",
    "tokenizer.fit_on_texts(input_ngram_sequences + output_ngram_sequences)\n",
    "\n",
    "# Convert text sequences to integer sequences\n",
    "input_sequences = tokenizer.texts_to_sequences(input_ngram_sequences)\n",
    "output_sequences = tokenizer.texts_to_sequences(output_ngram_sequences)\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_seq_length = max(len(seq) for seq in input_sequences + output_sequences)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length, padding='post')\n",
    "output_sequences = pad_sequences(output_sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Define encoder input, decoder input, and decoder output\n",
    "encoder_input = input_sequences\n",
    "decoder_input = np.zeros_like(output_sequences)\n",
    "decoder_input[:, 1:] = output_sequences[:, :-1]\n",
    "decoder_output = np.eye(vocab_size)[output_sequences]\n",
    "\n",
    "# Define the Seq2Seq model\n",
    "latent_dim = 128\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(vocab_size, latent_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, dropout=dropout_rate, recurrent_dropout=dropout_rate)\n",
    "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "encoder_dropout = Dropout(dropout_rate)\n",
    "encoder_dropout_output = encoder_dropout(state_h)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(vocab_size, latent_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=dropout_rate, recurrent_dropout=dropout_rate)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dropout = Dropout(dropout_rate)\n",
    "decoder_dropout_outputs = decoder_dropout(decoder_outputs)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_dropout_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Define K-fold cross-validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Define RLHF parameters\n",
    "rl_epochs = 5  # Number of RL epochs\n",
    "rl_learning_rate = 0.001  # RL learning rate\n",
    "rl_batch_size = 64  # RL batch size\n",
    "reinforce_reward = 1  # Reward value for positive reinforcement\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(input_sequences)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    x_train, x_val = input_sequences[train_indices], input_sequences[val_indices]\n",
    "    y_train, y_val = decoder_output[train_indices], decoder_output[val_indices]\n",
    "    decoder_input_train, decoder_input_val = decoder_input[train_indices], decoder_input[val_indices]\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define early stopping callback\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    # Train the model using supervised learning\n",
    "    history = model.fit([x_train, decoder_input_train], y_train,\n",
    "                        validation_data=([x_val, decoder_input_val], y_val),\n",
    "                        epochs=10, batch_size=64, callbacks=[early_stop])\n",
    "\n",
    "    # Apply RLHF\n",
    "    for rl_epoch in range(rl_epochs):\n",
    "        # Generate translations using the current model\n",
    "        predictions = model.predict([x_train, decoder_input_train])\n",
    "        decoded_sequences = []\n",
    "        for prediction in predictions:\n",
    "            decoded_sequence = []\n",
    "            for token in prediction:\n",
    "                sampled_token_index = np.argmax(token)\n",
    "                sampled_token = tokenizer.index_word.get(sampled_token_index, '')\n",
    "                if sampled_token != '':\n",
    "                    decoded_sequence.append(sampled_token)\n",
    "            decoded_sequences.append(decoded_sequence)\n",
    "\n",
    "        # Calculate rewards based on BLEU score or other metrics\n",
    "        # rewards = calculate_rewards(decoded_sequences, ground_truth_sequences)\n",
    "\n",
    "        # Prepare RL data by converting sequences to integer sequences\n",
    "        rl_input_sequences = tokenizer.texts_to_sequences(decoded_sequences)\n",
    "        rl_input_sequences = pad_sequences(rl_input_sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "        # Train the model using RL\n",
    "        model.optimizer.lr.assign(rl_learning_rate)\n",
    "        model.fit([x_train, rl_input_sequences], y_train, epochs=1, batch_size=rl_batch_size)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate([x_val, decoder_input_val], y_val)\n",
    "    print(test_loss)\n",
    "    print(test_acc)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
